{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9c2e8f",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb80bd",
   "metadata": {},
   "source": [
    "## Problem (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8894ad3",
   "metadata": {},
   "source": [
    "### 1. Download(Generate) dynamics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a478ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating a 1D CLG model ensemble takes a long time\n",
    "# so will use a pre-computed ensemble of runs\n",
    "# done on my local machine, which can be downloaded from the cell above\n",
    "!wget -O clg_finals_dynamics_10000samples.npy https://github.com/kimmw3002/CLP_HW1/raw/refs/heads/main/clg_finals_dynamics_10000samples.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c186c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clg.py\n",
    "# on windows, make this a separate .py file to avoid issues with multiprocessing\n",
    "# can be run directly as a cell in colab\n",
    "\n",
    "# simulating a 1D CLG model ensemble takes a long time\n",
    "# so will use a pre-computed ensemble of runs\n",
    "# done on my local machine, which can be downloaded from the cell above\n",
    "# in theory, you could run this to generate the ensemble on colab\n",
    "import numpy as np\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# ---------- single‑run simulator ----------\n",
    "def run_CLG_until_absorbing(L, N, max_steps=1_000_000, rng=None):\n",
    "    rng = rng or np.random.default_rng()\n",
    "    conf = np.zeros(L, dtype=np.int8)\n",
    "    conf[rng.choice(L, N, replace=False)] = 1\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        occ = conf == 1\n",
    "        left_occ   = np.roll(occ,  1)\n",
    "        right_occ  = np.roll(occ, -1)\n",
    "        left_emp   = np.roll(conf,  1) == 0\n",
    "        right_emp  = np.roll(conf, -1) == 0\n",
    "        movable = np.where(occ & (left_occ | right_occ) & (left_emp | right_emp))[0]\n",
    "        if movable.size == 0:\n",
    "            return conf\n",
    "        i = rng.choice(movable)\n",
    "        targets = [(i - 1) % L] if left_emp[i] else []\n",
    "        if right_emp[i]:\n",
    "            targets.append((i + 1) % L)\n",
    "        j = rng.choice(targets)\n",
    "        conf[j], conf[i] = 1, 0\n",
    "    warnings.warn(\"max_steps reached before absorbing\", RuntimeWarning)\n",
    "    return conf\n",
    "\n",
    "# ---------- helper for the pool ----------\n",
    "def _worker(args):\n",
    "    L, N, max_steps, seed = args\n",
    "    return run_CLG_until_absorbing(L, N, max_steps, rng=np.random.default_rng(seed))\n",
    "\n",
    "# ---------- public ensemble ----------\n",
    "def run_ensemble_cpu(L, N, runs, max_steps=1_000_000, n_workers=None, seed=None):\n",
    "    master = np.random.default_rng(seed)\n",
    "    seeds  = master.integers(0, 2**63 - 1, size=runs)\n",
    "    args   = [(L, N, max_steps, s) for s in seeds]\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as pool:\n",
    "        finals = list(pool.map(_worker, args, chunksize=1))\n",
    "    return np.array(finals)            # shape (runs, L)\n",
    "\n",
    "# ---------- run the ensemble ----------\n",
    "if __name__ == \"__main__\":\n",
    "    L, rho, R = 10_000, 0.4, 10000    # lattice, density, #runs\n",
    "    finals = run_ensemble_cpu(L, int(L*rho), R, n_workers=None, seed=42)\n",
    "    np.save(\"clg_finals_dynamics_10000samples.npy\", finals)\n",
    "    print(\"CPU ensemble done & saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b59a80",
   "metadata": {},
   "source": [
    "### 2. Generate combinatorics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd37fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to clg_finals_combinatorics_10000samples.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_dataset2_fast(L, N, C):\n",
    "    \"\"\"\n",
    "    Returns a (C, L) array of 0/1 rows, each with N ones and no two 1s adjacent.\n",
    "    \"\"\"\n",
    "    M = L - N + 1\n",
    "    # 1) pick N “slots” in [0..M-1] per row\n",
    "    rand = np.random.rand(C, M)\n",
    "    idxs = np.argpartition(rand, N-1, axis=1)[:, :N]\n",
    "    idxs.sort(axis=1)\n",
    "\n",
    "    # 2) shift so no two 1’s touch\n",
    "    offsets   = np.arange(N)\n",
    "    positions = idxs + offsets  # (C, N)\n",
    "\n",
    "    # 3) scatter into zero array\n",
    "    data = np.zeros((C, L), dtype=np.int8)\n",
    "    rows = np.arange(C)[:, None]\n",
    "    data[rows, positions] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "L, N, C = 10000, 4000, 10000   # parameters for the dataset\n",
    "data = run_dataset2_fast(L, N, C)\n",
    "# save to .npy\n",
    "np.save(\"clg_finals_combinatorics_10000samples.npy\", data)\n",
    "print(f\"Saved to clg_finals_combinatorics_10000samples.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ce615",
   "metadata": {},
   "source": [
    "## Problem (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9b0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamics CID:      mean = 0.095458, std = 0.001225\n",
      "Combinatorics CID: mean = 0.106395, std = 0.000810\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zlib\n",
    "\n",
    "def compress_lza(data_bytes):\n",
    "    \"\"\"\n",
    "    Placeholder LZ77‐based compressor wrapper.\n",
    "    Currently uses zlib (DEFLATE) for fast C‐based compression.\n",
    "    Returns compressed byte‐length.\n",
    "    \"\"\"\n",
    "    return len(zlib.compress(data_bytes))\n",
    "\n",
    "def compute_cid_stats(dataset, compressor):\n",
    "    \"\"\"\n",
    "    Returns (mean_cid, std_cid) for CID = compressed_length / original_length\n",
    "    computed over each row of `dataset` (shape (C, L)).\n",
    "    \"\"\"\n",
    "    C, L = dataset.shape\n",
    "    cids = np.empty(C, dtype=float)\n",
    "    for i in range(C):\n",
    "        row_bytes   = dataset[i].astype(np.uint8).tobytes()\n",
    "        compressed  = compressor(row_bytes)\n",
    "        cids[i]     = compressed / len(row_bytes)\n",
    "    return cids.mean(), cids.std()\n",
    "\n",
    "dyn_file  = \"clg_finals_dynamics_10000samples.npy\"\n",
    "comb_file = \"clg_finals_combinatorics_10000samples.npy\"\n",
    "\n",
    "# load datasets\n",
    "dynamics      = np.load(dyn_file)\n",
    "combinatorics = np.load(comb_file)\n",
    "\n",
    "# compute mean and standard deviation of CID\n",
    "mean_dyn,  std_dyn  = compute_cid_stats(dynamics,      compress_lza)\n",
    "mean_comb, std_comb = compute_cid_stats(combinatorics, compress_lza)\n",
    "\n",
    "print(f\"Dynamics CID:      mean = {mean_dyn:.6f}, std = {std_dyn:.6f}\")\n",
    "print(f\"Combinatorics CID: mean = {mean_comb:.6f}, std = {std_comb:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd4d18",
   "metadata": {},
   "source": [
    "- **Dynamics**: $\\bar x_1 = 0.095458$, $\\sigma_1 = 0.001225$, $N_1 = 10000$  \n",
    "- **Combinatorics**: $\\bar x_2 = 0.106395$, $\\sigma_2 = 0.000810$, $N_2 = 10000$  \n",
    "\n",
    "---\n",
    "\n",
    "1. **Standard error of each mean**\n",
    "\n",
    "   $$\n",
    "   \\mathrm{SE}_1 = \\frac{\\sigma_1}{\\sqrt{N_1}}\n",
    "     = \\frac{0.001225}{\\sqrt{10000}}\n",
    "     = 1.225\\times10^{-5},\n",
    "   \\quad\n",
    "   \\mathrm{SE}_2 = \\frac{\\sigma_2}{\\sqrt{N_2}}\n",
    "     = \\frac{0.000810}{\\sqrt{10000}}\n",
    "     = 8.10\\times10^{-6}.\n",
    "   $$\n",
    "\n",
    "2. **Standard error of the difference**\n",
    "\n",
    "   $$\n",
    "   \\mathrm{SE}_\\Delta\n",
    "     = \\sqrt{\\mathrm{SE}_1^2 + \\mathrm{SE}_2^2}\n",
    "     = \\sqrt{(1.225\\times10^{-5})^2 + (8.10\\times10^{-6})^2}\n",
    "     \\approx 1.469\\times10^{-5}.\n",
    "   $$\n",
    "\n",
    "3. **Difference of means**\n",
    "\n",
    "   $$\n",
    "   \\Delta \\mu\n",
    "     = \\bar x_2 - \\bar x_1\n",
    "     = 0.106395 - 0.095458\n",
    "     = 0.010937.\n",
    "   $$\n",
    "\n",
    "4. **Z-score**\n",
    "\n",
    "   $$\n",
    "   z\n",
    "     = \\frac{\\Delta \\mu}{\\mathrm{SE}_\\Delta}\n",
    "     = \\frac{0.010937}{1.469\\times10^{-5}}\n",
    "     \\approx 745.\n",
    "   $$\n",
    "\n",
    "5. **Two-sided $p$-value**\n",
    "\n",
    "   $$\n",
    "   p\n",
    "     = 2\\,(1 - \\Phi(z))\n",
    "     \\approx 0.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "The difference in average CID ($\\Delta\\mu\\approx0.01094$) corresponds to a $z$-score of about 745—hundreds of standard errors apart—so it remains overwhelmingly statistically significant. This proves that the dynamically reached ensemble is different from the combinatorics based ensemble generation. The dynamically reached states are not a uniform probability ensemble of all combinatorically possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298a42b",
   "metadata": {},
   "source": [
    "## Problem (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97b9fa",
   "metadata": {},
   "source": [
    "### 1. Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72a197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class CLGModel(nn.Module):\n",
    "    def __init__(self, L = 10000):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, stride = 1, padding=1),  # Convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # Max pooling layer\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride = 1, padding=1),  # Second convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # Second max pooling layer\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride = 1, padding=1),  # Third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # Third max pooling layer\n",
    "\n",
    "            nn.Flatten(),  # Flatten the output for the fully connected layers\n",
    "            nn.Linear(64 * (self.L // 8), 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # Second fully connected layer\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.L)  # Reshape input to (batch_size, 1, L)\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e728371",
   "metadata": {},
   "source": [
    "### 2. Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2266aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 0.1443 | Val Acc: 1.0000\n",
      "Epoch  2 | Train Loss: 0.0001 | Val Acc: 1.0000\n",
      "Epoch  3 | Train Loss: 0.0000 | Val Acc: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = CLGModel(L=10000)  # Initialize the model with L=10000\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the device (GPU or CPU)\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "dynamics = np.load(\"clg_finals_dynamics_10000samples.npy\")\n",
    "combinatorics = np.load(\"clg_finals_combinatorics_10000samples.npy\")\n",
    "\n",
    "# Combine the datasets and create labels\n",
    "X = np.concatenate((dynamics, combinatorics), axis=0)\n",
    "y = np.concatenate((np.zeros(dynamics.shape[0]), np.ones(combinatorics.shape[0])), axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Split into training, validation, test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# convert numpy to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "# datasets & loaders\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch:2d} | Train Loss: {epoch_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# test evaluation\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb).argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
