{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df9c2e8f",
      "metadata": {
        "id": "df9c2e8f"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffb80bd",
      "metadata": {
        "id": "9ffb80bd"
      },
      "source": [
        "## Problem (a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8894ad3",
      "metadata": {
        "id": "f8894ad3"
      },
      "source": [
        "### 1. Download(Generate) dynamics dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a478ee7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a478ee7",
        "outputId": "0aa20987-61e1-42f5-d761-913caa919c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-07 11:55:37--  https://github.com/kimmw3002/CLP_HW1/raw/refs/heads/main/clg_finals_dynamics_10000samples.npy\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kimmw3002/CLP_HW1/refs/heads/main/clg_finals_dynamics_10000samples.npy [following]\n",
            "--2025-05-07 11:55:38--  https://raw.githubusercontent.com/kimmw3002/CLP_HW1/refs/heads/main/clg_finals_dynamics_10000samples.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100000128 (95M) [application/octet-stream]\n",
            "Saving to: ‘clg_finals_dynamics_10000samples.npy’\n",
            "\n",
            "clg_finals_dynamics 100%[===================>]  95.37M   363MB/s    in 0.3s    \n",
            "\n",
            "2025-05-07 11:55:45 (363 MB/s) - ‘clg_finals_dynamics_10000samples.npy’ saved [100000128/100000128]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# simulating a 1D CLG model ensemble takes a long time\n",
        "# so will use a pre-computed ensemble of runs\n",
        "# done on my local machine, which can be downloaded via:\n",
        "!wget -O clg_finals_dynamics_10000samples.npy https://github.com/kimmw3002/CLP_HW1/raw/refs/heads/main/clg_finals_dynamics_10000samples.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c186c5",
      "metadata": {
        "id": "92c186c5"
      },
      "outputs": [],
      "source": [
        "# clg.py\n",
        "# on windows, make this a separate .py file to avoid issues with multiprocessing\n",
        "# can be run directly as a cell in colab\n",
        "\n",
        "# simulating a 1D CLG model ensemble takes a long time\n",
        "# so will use a pre-computed ensemble of runs\n",
        "# done on my local machine, which can be downloaded from the cell above\n",
        "# in theory, you could run this to generate the ensemble on colab\n",
        "import numpy as np\n",
        "import warnings\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# ---------- single‑run simulator ----------\n",
        "def run_CLG_until_absorbing(L, N, max_steps=1_000_000, rng=None):\n",
        "    rng = rng or np.random.default_rng()\n",
        "    conf = np.zeros(L, dtype=np.int8)\n",
        "    conf[rng.choice(L, N, replace=False)] = 1\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        occ = conf == 1\n",
        "        left_occ   = np.roll(occ,  1)\n",
        "        right_occ  = np.roll(occ, -1)\n",
        "        left_emp   = np.roll(conf,  1) == 0\n",
        "        right_emp  = np.roll(conf, -1) == 0\n",
        "        movable = np.where(occ & (left_occ | right_occ) & (left_emp | right_emp))[0]\n",
        "        if movable.size == 0:\n",
        "            return conf\n",
        "        i = rng.choice(movable)\n",
        "        targets = [(i - 1) % L] if left_emp[i] else []\n",
        "        if right_emp[i]:\n",
        "            targets.append((i + 1) % L)\n",
        "        j = rng.choice(targets)\n",
        "        conf[j], conf[i] = 1, 0\n",
        "    warnings.warn(\"max_steps reached before absorbing\", RuntimeWarning)\n",
        "    return conf\n",
        "\n",
        "# ---------- helper for the pool ----------\n",
        "def _worker(args):\n",
        "    L, N, max_steps, seed = args\n",
        "    return run_CLG_until_absorbing(L, N, max_steps, rng=np.random.default_rng(seed))\n",
        "\n",
        "# ---------- public ensemble ----------\n",
        "def run_ensemble_cpu(L, N, runs, max_steps=1_000_000, n_workers=None, seed=None):\n",
        "    master = np.random.default_rng(seed)\n",
        "    seeds  = master.integers(0, 2**63 - 1, size=runs)\n",
        "    args   = [(L, N, max_steps, s) for s in seeds]\n",
        "    with ProcessPoolExecutor(max_workers=n_workers) as pool:\n",
        "        finals = list(pool.map(_worker, args, chunksize=1))\n",
        "    return np.array(finals)            # shape (runs, L)\n",
        "\n",
        "# ---------- run the ensemble ----------\n",
        "if __name__ == \"__main__\":\n",
        "    L, rho, R = 10_000, 0.4, 10000    # lattice, density, #runs\n",
        "    finals = run_ensemble_cpu(L, int(L*rho), R, n_workers=None, seed=42)\n",
        "    np.save(\"clg_finals_dynamics_10000samples.npy\", finals)\n",
        "    print(\"CPU ensemble done & saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b59a80",
      "metadata": {
        "id": "87b59a80"
      },
      "source": [
        "### 2. Generate combinatorics dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bd37fea2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd37fea2",
        "outputId": "f3213f12-c9c1-4c5e-e294-c53ae13eecc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to clg_finals_combinatorics_10000samples.npy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def run_dataset2_fast(L, N, C):\n",
        "    \"\"\"\n",
        "    Returns a (C, L) array of 0/1 rows, each with N ones and no two 1s adjacent.\n",
        "    \"\"\"\n",
        "    M = L - N + 1\n",
        "    # 1) pick N “slots” in [0..M-1] per row\n",
        "    rand = np.random.rand(C, M)\n",
        "    idxs = np.argpartition(rand, N-1, axis=1)[:, :N]\n",
        "    idxs.sort(axis=1)\n",
        "\n",
        "    # 2) shift so no two 1’s touch\n",
        "    offsets   = np.arange(N)\n",
        "    positions = idxs + offsets  # (C, N)\n",
        "\n",
        "    # 3) scatter into zero array\n",
        "    data = np.zeros((C, L), dtype=np.int8)\n",
        "    rows = np.arange(C)[:, None]\n",
        "    data[rows, positions] = 1\n",
        "\n",
        "    return data\n",
        "\n",
        "L, N, C = 10000, 4000, 10000   # parameters for the dataset\n",
        "data = run_dataset2_fast(L, N, C)\n",
        "# save to .npy\n",
        "np.save(\"clg_finals_combinatorics_10000samples.npy\", data)\n",
        "print(f\"Saved to clg_finals_combinatorics_10000samples.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3ce615",
      "metadata": {
        "id": "5b3ce615"
      },
      "source": [
        "## Problem (b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac9b0fd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9b0fd6",
        "outputId": "f7e00823-eb62-4b4a-8fd8-5294da645e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dynamics CID:      mean = 0.095458, std = 0.001225\n",
            "Combinatorics CID: mean = 0.106414, std = 0.000825\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import zlib\n",
        "\n",
        "def compress_lza(data_bytes):\n",
        "    \"\"\"\n",
        "    Placeholder LZ77-based compressor wrapper.\n",
        "    Currently uses zlib (DEFLATE) for fast C-based compression.\n",
        "    Returns compressed byte-length.\n",
        "    \"\"\"\n",
        "    return len(zlib.compress(data_bytes))\n",
        "\n",
        "def compute_cid_stats(dataset, compressor):\n",
        "    \"\"\"\n",
        "    Returns (mean_cid, std_cid) for CID = compressed_length / original_length\n",
        "    computed over each row of `dataset` (shape (C, L)).\n",
        "    \"\"\"\n",
        "    C, L = dataset.shape\n",
        "    cids = np.empty(C, dtype=float)\n",
        "    for i in range(C):\n",
        "        row_bytes   = dataset[i].astype(np.uint8).tobytes()\n",
        "        compressed  = compressor(row_bytes)\n",
        "        cids[i]     = compressed / len(row_bytes)\n",
        "    return cids.mean(), cids.std()\n",
        "\n",
        "dyn_file  = \"clg_finals_dynamics_10000samples.npy\"\n",
        "comb_file = \"clg_finals_combinatorics_10000samples.npy\"\n",
        "\n",
        "# load datasets\n",
        "dynamics      = np.load(dyn_file)\n",
        "combinatorics = np.load(comb_file)\n",
        "\n",
        "# compute mean and standard deviation of CID\n",
        "mean_dyn,  std_dyn  = compute_cid_stats(dynamics,      compress_lza)\n",
        "mean_comb, std_comb = compute_cid_stats(combinatorics, compress_lza)\n",
        "\n",
        "print(f\"Dynamics CID:      mean = {mean_dyn:.6f}, std = {std_dyn:.6f}\")\n",
        "print(f\"Combinatorics CID: mean = {mean_comb:.6f}, std = {std_comb:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37bd4d18",
      "metadata": {
        "id": "37bd4d18"
      },
      "source": [
        "1. **Standard error of each mean**\n",
        "\n",
        "   $$\n",
        "   \\mathrm{SE}_1 = \\frac{\\sigma_1}{\\sqrt{N_1}}\n",
        "     = \\frac{0.001225}{\\sqrt{10000}}\n",
        "     = 1.225\\times10^{-5},\n",
        "   \\quad\n",
        "   \\mathrm{SE}_2 = \\frac{\\sigma_2}{\\sqrt{N_2}}\n",
        "     = \\frac{0.000825}{\\sqrt{10000}}\n",
        "     = 8.25\\times10^{-6}.\n",
        "   $$\n",
        "\n",
        "2. **Standard error of the difference**\n",
        "\n",
        "   $$\n",
        "   \\mathrm{SE}_\\Delta\n",
        "     = \\sqrt{\\mathrm{SE}_1^2 + \\mathrm{SE}_2^2}\n",
        "     = \\sqrt{(1.225\\times10^{-5})^2 + (8.25\\times10^{-6})^2}\n",
        "     \\approx 1.477\\times10^{-5}.\n",
        "   $$\n",
        "\n",
        "3. **Difference of means**\n",
        "\n",
        "   $$\n",
        "   \\Delta \\mu\n",
        "     = \\bar x_2 - \\bar x_1\n",
        "     = 0.106414 - 0.095458\n",
        "     = 0.010956.\n",
        "   $$\n",
        "\n",
        "4. **Z-score**\n",
        "\n",
        "   $$\n",
        "   z\n",
        "     = \\frac{\\Delta \\mu}{\\mathrm{SE}_\\Delta}\n",
        "     = \\frac{0.010956}{1.477\\times10^{-5}}\n",
        "     \\approx 741.8.\n",
        "   $$\n",
        "\n",
        "5. **Two-sided \\(p\\)-value**\n",
        "\n",
        "   $$\n",
        "   p\n",
        "     = 2\\,(1 - \\Phi(z))\n",
        "     \\approx 0.\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion:**  \n",
        "The difference remains hundreds of standard errors apart—overwhelmingly significant. This proves that the dynamically reached ensemble is different from the combinatorics based ensemble generation. The dynamically reached states are not a uniform probability ensemble of all combinatorically possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2298a42b",
      "metadata": {
        "id": "2298a42b"
      },
      "source": [
        "## Problem (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e054fd",
      "metadata": {
        "id": "77e054fd"
      },
      "source": [
        "We change the hyperparameter `n_blocks` to test the performance of the 1d CNN. One \"block\" is composed with `Conv1d` + `ReLU` + `MaxPool1d`. For the first block, the number of channels is selected as 16, and for each another block, the number of channel doubles. Other hyperparameters are the most typically used ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6528c59d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6528c59d",
        "outputId": "6d1f81ba-d04c-4964-d066-282f7288cf48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "─── 1 BLOCK ─────────────────────\n",
            "Epoch  1 | Train Loss 1.0421 | Val Acc 0.4975\n",
            "Epoch  2 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  3 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  4 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  5 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  6 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  7 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  8 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  9 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch 10 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Test Accuracy: 0.4855\n",
            "\n",
            "─── 2 BLOCKS ─────────────────────\n",
            "Epoch  1 | Train Loss 0.8416 | Val Acc 0.4975\n",
            "Epoch  2 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  3 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  4 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  5 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  6 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  7 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  8 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch  9 | Train Loss 0.6932 | Val Acc 0.4975\n",
            "Epoch 10 | Train Loss 0.6932 | Val Acc 0.5025\n",
            "Test Accuracy: 0.5145\n",
            "\n",
            "─── 3 BLOCKS ─────────────────────\n",
            "Epoch  1 | Train Loss 0.0794 | Val Acc 1.0000\n",
            "Epoch  2 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  3 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  4 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  5 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  6 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  7 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  8 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch  9 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Epoch 10 | Train Loss 0.0000 | Val Acc 1.0000\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Full, self-contained script to compare 1, 2, 3 convolutional “blocks”.\n",
        "\n",
        "A *block* = Conv1d(kernel_size =3, stride = 1, padding = 1)\n",
        "         → ReLU\n",
        "         → MaxPool1d(kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "Conv channel width doubles after every block.\n",
        "After n_blocks pools the 1-D length shrinks to  L // (2**n_blocks).\n",
        "\n",
        "The first Linear therefore needs:\n",
        "    flat_dim = (base_ch * 2**(n_blocks-1)) * (L // 2**n_blocks)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ───────────────────────────────── hyper‑params ──────────────────────────────\n",
        "L           = 10_000          # input sequence length\n",
        "base_ch     = 16              # channels of the first conv\n",
        "batch_size  = 64\n",
        "epochs      = 10\n",
        "lr          = 1e-3\n",
        "block_list  = (1, 2, 3)       # experiment sweep\n",
        "# ───────────────────────────────── reproducibility ────────────────────────────\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "# ───────────────────────────────────── model ──────────────────────────────────\n",
        "class CLGModel(nn.Module):\n",
        "    def __init__(self, L: int, n_blocks: int, base_ch: int = 16):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_ch, out_ch = 1, base_ch\n",
        "\n",
        "        for _ in range(n_blocks):\n",
        "            layers.extend([\n",
        "                nn.Conv1d(in_channels=in_ch,\n",
        "                          out_channels=out_ch,\n",
        "                          kernel_size=3,  # explicit\n",
        "                          stride=1,       # explicit\n",
        "                          padding=1),     # explicit\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool1d(kernel_size=2,  # explicit\n",
        "                             stride=2,       # explicit\n",
        "                             padding=0)      # explicit\n",
        "            ])\n",
        "            in_ch, out_ch = out_ch, out_ch * 2  # double channels\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "        seq_len_after = L // (2 ** n_blocks)\n",
        "        channels_after = base_ch * (2 ** (n_blocks - 1))\n",
        "        flat_dim = channels_after * seq_len_after\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(flat_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2)              # raw logits for CrossEntropyLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 1, -1)       # (B,1,L)\n",
        "        x = self.conv(x)\n",
        "        return self.cls(x)\n",
        "\n",
        "# ───────────────────────────────────── data ───────────────────────────────────\n",
        "dynamics       = np.load(\"clg_finals_dynamics_10000samples.npy\")\n",
        "combinatorics  = np.load(\"clg_finals_combinatorics_10000samples.npy\")\n",
        "\n",
        "X = np.concatenate((dynamics, combinatorics), axis=0)\n",
        "y = np.concatenate((np.zeros(len(dynamics)), np.ones(len(combinatorics))), axis=0)\n",
        "\n",
        "indices = np.random.permutation(len(X))\n",
        "X, y = X[indices], y[indices]\n",
        "\n",
        "train_end = int(0.8 * len(X))\n",
        "val_end   = int(0.9 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_end],          y[:train_end]\n",
        "X_val,   y_val   = X[train_end:val_end],   y[train_end:val_end]\n",
        "X_test,  y_test  = X[val_end:],            y[val_end:]\n",
        "\n",
        "def to_loader(Xn, yn, shuffle=False):\n",
        "    X_t = torch.from_numpy(Xn).float()\n",
        "    y_t = torch.from_numpy(yn).long()\n",
        "    ds  = TensorDataset(X_t, y_t)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
        "\n",
        "train_loader = to_loader(X_train, y_train, shuffle=True)\n",
        "val_loader   = to_loader(X_val,   y_val)\n",
        "test_loader  = to_loader(X_test,  y_test)\n",
        "\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ────────────────────────────── helpers ───────────────────────────────────────\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss = criterion(model(xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * xb.size(0)\n",
        "    return running / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = tot = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        correct += (model(xb).argmax(1) == yb).sum().item()\n",
        "        tot     += yb.size(0)\n",
        "    return correct / tot\n",
        "\n",
        "# ───────────────────────────── experiment loop ───────────────────────────────\n",
        "for n_blocks in block_list:\n",
        "    model = CLGModel(L=L, n_blocks=n_blocks, base_ch=base_ch).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"\\n─── {n_blocks} BLOCK{'S' if n_blocks>1 else ''} ─────────────────────\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr_loss = train_one_epoch(model, train_loader, optimizer)\n",
        "        val_acc = accuracy(model, val_loader)\n",
        "        print(f\"Epoch {epoch:2d} | Train Loss {tr_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
        "\n",
        "    tst_acc = accuracy(model, test_loader)\n",
        "    print(f\"Test Accuracy: {tst_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c94715e2",
      "metadata": {
        "id": "c94715e2"
      },
      "source": [
        "The result is surprising: for `n_blocks` value 1 or 2, the model struggles to learn - the accuracy is close to 50%, which suggests the model is nearly random guessing. But for `n_blocks` value 3, the model suddenly classifies the two datasets to a perfect level! This suggests that the characteristic difference of the two datasets is \"long-ranged\" so at least 3 blocks of convolution + max pooling is required to extract the feature that discriminates the two datasets."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
